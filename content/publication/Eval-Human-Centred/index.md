---
title: 'Evaluating Human-Centered AI Explanations: Introduction of an XAI Evaluation Framework for Fact-Checking'
subtitle: 'Vera Schmitt, Balázs Patrik Csomor, Joachim Meyer, Luis-Felipe Villa-Areas, Charlott Jakob, Tim Polzehl, Sebastian Möller - 3rd ACM International Workshop on Multimedia AI against Disinformation'

# Authors
# If you created a profile for a user (e.g. the default `admin` user), write the username (folder name) here
# and it will be replaced with their full name and linked to their profile.
authors:
    - Dr. Vera Schmitt
    - Balázs Patrik Csomor
    - Joachim Meyer
    - Luis-Felipe Villa-Areas
    - Charlott Jakob
    - Tim Polzehl
    - Prof. Sebastian Möller
# Author notes (optional)
author_notes: 

date: '2024-06-10T00:00:00Z'
doi: ''

# Schedule page publish date (NOT publication's date).
publishDate: '2017-01-01T00:00:00Z'

# Publication type.
# Accepts a single type but formatted as a YAML list (for Hugo requirements).
# Enter a publication type from the CSL standard.
publication_types: ['paper-conference']

# Publication name and optional abbreviated publication name.
publication: In *Proceedings of the 3rd ACM International Workshop on Multimedia AI against Disinformation*
publication_short: In *Proceedings of the 3rd ACM International Workshop on Multimedia AI against Disinformation*

abstract: The rapidly increasing amount of online information and the ad-vent of Generative Artificial Intelligence (GenAI) make the manualverification of information impractical. Consequently, AI systemsare deployed to detect disinformation and deepfakes. Prior studieshave indicated that combining AI and human capabilities yieldsenhanced performance in detecting disinformation. Furthermore,the European Union (EU) AI Act mandates human supervision forAI applications in areas impacting essential human rights, like free-dom of speech, necessitating that AI systems be transparent andprovide adequate explanations to ensure comprehensibility. Exten-sive research has been conducted on incorporating explainability(XAI) attributes to augment AI transparency, yet these often miss ahuman-centric assessment. The effectiveness of such explanationsalso varies with the user’s prior knowledge and personal attributes.Therefore, we developed a framework for validating XAI featuresfor the collaborative human-AI fact-checking task. The frameworkallows the testing of XAI features with objective and subjectiveevaluation dimensions and follows human-centric design principleswhen displaying information about the AI system to the users. Theframework was tested in a crowdsourcing experiment with 433participants, including 406 crowdworkers and 27 journalists for thecollaborative disinformation detection task. The tested XAI featuresincrease the AI system’s perceived usefulness, understandability,and trust. With this publication, the XAI evaluation framework ismade open source.
# Summary. An optional shortened abstract.
summary: 

tags: []

# Display this page in the Featured widget?
featured: true

# Custom links (uncomment lines below)
# links:
# - name: Custom Link
#   url: http://example.org

url_pdf: 'https://dl.acm.org/doi/pdf/10.1145/3643491.3660283'
url_code: ''
url_dataset: ''
url_poster: ''
url_project: ''
url_slides: ''
url_source: ''
url_video: ''

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Associated Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `internal-project` references `content/project/internal-project/index.md`.
#   Otherwise, set `projects: []`.
projects: []

# Slides (optional).
#   Associate this publication with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides: "example"` references `content/slides/example/index.md`.
#   Otherwise, set `slides: ""`.
slides: ""
---


