---
title: 'Interview with BR: "Faktencheck mit KI? Prüfen Sie die Antwort!"'
date: 2025-04-07
---

In a BR24 article highlighting the dangers of AI fact-checking, expert Dr. Vera Schmitt explains that chatbots are fundamentally unreliable for this task because they are designed to generate statistically probable text rather than verify truthfulness.
<!--more-->

The article from BR24 ["Faktencheck mit KI? Prüfen Sie die Antwort!"](https://www.br.de/nachrichten/netzwelt/faktencheck-mit-ki-pruefen-sie-die-antwort,UpgkXye) examines the profound unreliability of current AI chatbots when used for fact-checking complex topics. Featuring expert analyses from [Stefan Voss (dpa)](https://innovation.dpa.com/author/stefan-voss/), [Prof. Gitta Kutyniok (LMU)](https://www.ai.math.uni-muenchen.de/members/professor/kutyniok/index.html), and [Dr. Vera Schmitt (TU Berlin)](https://veraschmitt.github.io/) the report details how these models are designed to generate statistically plausible text rather than to verify truthfulness and factuality, making them fundamentally unsuited for verification tasks. It delves into the primary failure modes, including the generation of invented facts ('hallucinations/fabrications'), the reinforcement of user biases, and the models' training on opaque datasets containing misinformation. Ultimately, the report concludes that while chatbots can be research aids, they lack the necessary mechanisms for critical evaluation, placing the entire burden of verification on the user.
